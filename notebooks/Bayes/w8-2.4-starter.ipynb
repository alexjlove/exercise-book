{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For today's lab we will build a basic Bayesian regression using PyMC. We're going to use a data-set procured from a credit card company with basic demographic data, and a limit balance. The later will be the target for our regression, and we will use the categorical column: sex, education, marriage, and the numerical column age, as our features. \n",
    "\n",
    "The point of the lab will be to have greater familiarity with the PyMC environment as well as see how constructing a Bayesian regression is different from a classical multiple variable regression model. You'll note while doing the lab that building a Bayesian regression is often more involved, however, you'll also note that Bayesian regressions will give you, the data scientist more control over assumptions in the analytic model itself. \n",
    "\n",
    "With this added complexity, comes a cost in terms of actually executing/estimating the parameters of your model, and thus you'll have to leverage a powerful procedure (Monte-Carlo Markov Chains) to actually compute the regression. You should be excited, as with a little more work, and learning in this sub-area of data science can quickly lead to topics currently within the frontier such as deep learning. \n",
    "\n",
    "Let's begin!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be working with our second UCI Machine Learning Repository data set. \n",
    "\n",
    "The description of the data is as follows from the website (https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients#):\n",
    "\n",
    "    This research aimed at the case of customersâ€™ default payments in Taiwan and compares the predictive accuracy of probability of default among six data mining methods. From the perspective of risk management, the result of predictive accuracy of the estimated probability of default will be more valuable than the binary result of classification - credible or not credible clients. Because the real probability of default is unknown, this study presented the novel â€œSorting Smoothing Methodâ€ to estimate the real probability of default. With the real probability of default as the response variable (Y), and the predictive probability of default as the independent variable (X), the simple linear regression result (Y = A + BX) shows that the forecasting model produced by artificial neural network has the highest coefficient of determination; its regression intercept (A) is close to zero, and regression coefficient (B) to one. Therefore, among the six data mining techniques, artificial neural network is the only one that can accurately estimate the real probability of default.\n",
    "    \n",
    "In this lab we're going to see what relationship simple demographics like sex, education, marriage status, and sex identifier have to the limit balance columns, but we'll build the regression model via the Bayesian paradigm and with PyMC, so you'll get more practice with this library and the associated methodologies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load up the default data , we will be doing a MLR on the Balance with simple demographics, sex, education, marriage, and age as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import math\n",
    "import pylab as py\n",
    "import sys\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the features of our simple model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilize a simple list/array from Python's base language to house the column names for your features, label the list rhs (for \"right-hand side\"). \n",
    "\n",
    "Do the same for your target, and define the list \"target\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Assigning the distributions for our error term and constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your intercept and error term. Label the error term \"noise\" and the intercept term \"beta0\". Please read the docs to understand how to call both the normal and uniform distirbution:  https://pymc-devs.github.io/pymc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "import pymc as pm\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigning the distribution for our parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define two arrays b, and x, as an empty numpy array of size 4. 'b' will contain our parameter values, and x will contain our numerical data inputs for the right-hand side. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have 4 features on your right hand side, so you must define each feature (and its distribution). For simplicity-sake, just define each feature as normal distribution. We're more concerned with you building the model through the first time. \n",
    "\n",
    "Quick Challenge Question: What is the only feature in our right-hand side that could appropriately be modeled by a normal distribution? What would be a better distribution for the others (Hint: We've mentioned it previously). \n",
    "\n",
    "You might be asking, why do we want to do this? Well a couple reasons, but one of the more important rationales is because of automation. It would be much easier to call a set of parameters if they were housed in an array/vector via a loop as this is simply just array[index_number], whereas if we just housed each parameter in their own variable, we'd have to explicitly call the variable name to get that particular value from memory. \n",
    "\n",
    "**Note**: As with the previous labs, just define all normal distributions with a mean 0 and a standard deviation of .0001.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing the Functional Form of our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've defined our parameters, we must now define our data. Again, for the sake of executing everything on the first run, let's assume the data is normally distributed (like the parameters) above, and put these data variables (there should be 4, one for each parameter variable) in the empty numpy array. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We previouslly said we'd come back to the two main model type classes, **deterministic** and **stochastic**. We're going to talk more about that now. Here is a reference \n",
    "\n",
    "https://pymc-devs.github.io/pymc/modelbuilding.html\n",
    "\n",
    "The docs explain it best: \n",
    "    \n",
    "    A Stochastic object represents a variable whose value is not completely determined by its parents, and a Deterministic object represents a variable that is entirely determined by its parents. In object-oriented programming parlance, Stochastic and Deterministic are subclasses of the Variable class, which only serves as a template for other classes and is never actually implemented in models.\n",
    "    \n",
    "So in plain english a deterministic model is one where all it's dependencies 'completely' define it. Think about a regression equation, what does it consist of? Parameters and data for features, a target, a constant term, and an error term. Have we accounted for all these components above in our specification? (Hint: Yes). So stands to reason we can use the deterministic class specifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Define a function, pred to be the assembled Bayesian regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're almost done! The last ingredient to our model is the target. Like the features, we must define the distribution for this object. Again, assume normality, and label the target 'y'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Construct a Graphical Representation of Our Bayesian Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So part of the value of building a Bayesian model is we're able to represent the model as a directed graph (See: http://algs4.cs.princeton.edu/42digraph/). Although for these simpler examples the importance of this feature may be less evident, as you mature in data science you may eventually build more complicated Bayesian models, including Bayesian networks, where this representation of a  model is more central to understanding the behavior of the algorithm.  \n",
    "\n",
    "We're going to practice building such a graph on our simple  Bayesian example. Construct a graphical representation of your Bayesian model - Hint you will need to have PyDot installed with GraphicViz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import pydot and pymc's graph method. You'll also need to import display_png from Ipython's display method. Please read the following docs and familiarize yourself with GraphicViz with the following blog post: https://pythonhaven.wordpress.com/2009/12/09/generating_graphs_with_pydot/, you should also look at the docs and the library Git: https://pypi.python.org/pypi/pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now call build a graphical representation of your model using pymc graph. Also, you now know where your \"labels\" show up when you define your model components, like parameters. As you build more complicated models, you'll grow to like these graphical representations, and they will help you organize the structure of the modeling objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the Monte-Carlo-Markov Chain Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the model through a MCMC procedure to get the estimations. For simplicity, use the input values of 10000, and 200 for the MCMC sample call. \n",
    "\n",
    "Obviouslly leverage the internet to get more background on MCMC (https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo). For more information on the MCMC method from PyMC, please read the following docs: https://pymc-devs.github.io/pymc/theory.html. The later is really good reading and explains the apparatus fairly well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print the Trace from the MCMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create trace histogram plots for your parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Parameter Estimates of your Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstruct the parameter values (like you would see in a traditional regression results table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've made it! If you have some extra time. Why don't you go back above and play around with the distributions, we know that categorical variables ARE NOT normally distributed. What is a more sound distribution? Find that distribution in the PyMC docs and redefine the appropriate variables in this way. Work through all the steps, making sure to fill in the appropriate parameters. Do we get drastically different numerical estimates now? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
